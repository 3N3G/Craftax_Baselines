{
    "architectures": [
        "Eagle3Speculator"
    ],
    "auto_map": {
        "": "eagle3.Eagle3SpeculatorConfig"
    },
    "draft_vocab_size": 32000,
    "eagle_aux_hidden_state_layer_ids": [
        35
    ],
    "has_no_defaults_at_init": false,
    "norm_before_residual": true,
    "speculators_config": {
        "algorithm": "eagle3",
        "default_proposal_method": "greedy",
        "proposal_methods": [
            {
                "accept_tolerance": 0.0,
                "proposal_type": "greedy",
                "speculative_tokens": 1,
                "verifier_accept_k": 1
            }
        ],
        "verifier": {
            "architectures": [
                "Qwen3ForCausalLM"
            ],
            "name_or_path": "Qwen/Qwen3-4B-Thinking-2507"
        }
    },
    "speculators_model_type": "extract_hidden_states",
    "speculators_version": "0.2.0.dev11",
    "torch_dtype": "bfloat16",
    "transformer_layer_config": {
        "attention_bias": false,
        "attention_dropout": 0.0,
        "head_dim": 128,
        "hidden_act": "silu",
        "hidden_size": 2560,
        "initializer_range": 0.02,
        "intermediate_size": 9728,
        "max_position_embeddings": 262144,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 5,
        "num_hidden_layers": 1,
        "num_key_value_heads": 5,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-06,
        "rope_scaling": null,
        "rope_theta": 5000000,
        "torch_dtype": "bfloat16",
        "use_cache": true,
        "vocab_size": 151936
    },
    "transformers_version": "4.57.1"
}